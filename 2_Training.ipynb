{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning\n",
    "---\n",
    "\n",
    "This notebook covers the training of the CNN-RNN model.  \n",
    "\n",
    "Contents:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "Customizing the training of the CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.\n",
    "\n",
    "The following papers are used to set up the most of parameters [1](https://arxiv.org/pdf/1502.03044.pdf) and [2](https://arxiv.org/pdf/1411.4555.pdf).\n",
    "\n",
    "\n",
    "Variables to setup:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from a file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model. [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but reasonable results can be seen in a matter of a few hours!\n",
    "- `save_every` - determines how often to save the model weights. After the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "**Model architecture** \n",
    "\n",
    "The model's architecture is a combination of CNN and RNN. CNN serves as encoder and RNN as a decoder. To keep things easy and at the same time utilize CNN performance a pre-trained ResNet-50 model is used with a modified last module where a fully connected layer with a size of 256 neurons is used. CNN neurons' weights are fixed apart from the last linear (fully connected one).\n",
    "The RNN starts with an embedding layer to transform the inputs (batch of words) into a fixed dimension,  particularly, each word is represented as a 256 length vector. It is the same as the encoder's output. Then, a concatenation of the encoder output and the embedding layer's input goes into LSTM cells with a hidden size of 512. The decoder's goal is to find the next word for the image description. Thus, the last layer is linear with the number of neurons equal to the vocabulary size. \n",
    "Different thresholds are used to set up a proper vocabulary size and to get rid of very rare words. Vocabulary size decreases as the threshold increases. For instance, with threshold 6 there are 8099 words in the vocabulary, 5 - 8855, 4 - 9955. It is decided to use 6 to keep the most useful and frequent words in the vocabulary. Besides, it makes the algorithm to run faster.\n",
    "\n",
    "**Image transformation:**\n",
    "\n",
    "Image transformation is an important part of a neural network pipeline. Some of them are necessary because images have different sizes that is why Resize(256), RandomCrop(224) are used. Because ResNet-50 uses normalized images, the normalization should be applied on images: Normalize((0.485, 0.456, 0.406). And finally, to make better generalization data augmentation is used: RandomRotation(10), ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), RandomHorizontalFlip(). And without any words, ToTensor() must be used to satisfy PyTorch's data representation.\n",
    "\n",
    "**Trainable parameters**\n",
    "\n",
    "The Resnet-50 is used for transfer learning to speed up the learning time and not to lose in performance. That is why all Encoder's layers are frozen apart from the last fully connected one which is supposed to learn the combination of features necessary for the decoder to capture the patterns. Besides, all Decoders' layers are taught parameters as well. \n",
    "\n",
    "**Optimizer**\n",
    "\n",
    "Adam [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer) with standard parameters is used as it is recomended in [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.89s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 929/414113 [00:00<01:28, 4645.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.85s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:32<00:00, 4497.12it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "import torch.optim\n",
    "\n",
    "batch_size = 64            # batch size\n",
    "vocab_threshold = 6        # minimum word count threshold\n",
    "vocab_from_file = True     # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# The learnable parameters of the model.\n",
    "params = list(decoder.parameters())\n",
    "\n",
    "# The optimizer.\n",
    "optimizer = optim.Adam(params=params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Model training\n",
    "\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well the model is doing, please have a look at how the training loss and perplexity evolve during training.\n",
    "\n",
    "However, this will not tell you if the model is overfitting on the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636) provided several approaches to minimizing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/6471], Loss: 3.8385, Perplexity: 46.4574\n",
      "Epoch [1/3], Step [200/6471], Loss: 3.4616, Perplexity: 31.8674\n",
      "Epoch [1/3], Step [300/6471], Loss: 3.3325, Perplexity: 28.0083\n",
      "Epoch [1/3], Step [400/6471], Loss: 3.1990, Perplexity: 24.5085\n",
      "Epoch [1/3], Step [500/6471], Loss: 3.1257, Perplexity: 22.7759\n",
      "Epoch [1/3], Step [600/6471], Loss: 3.0853, Perplexity: 21.8734\n",
      "Epoch [1/3], Step [700/6471], Loss: 2.9654, Perplexity: 19.4016\n",
      "Epoch [1/3], Step [800/6471], Loss: 5.1532, Perplexity: 172.9919\n",
      "Epoch [1/3], Step [900/6471], Loss: 2.9446, Perplexity: 19.0023\n",
      "Epoch [1/3], Step [1000/6471], Loss: 2.7136, Perplexity: 15.0836\n",
      "Epoch [1/3], Step [1100/6471], Loss: 2.9283, Perplexity: 18.6965\n",
      "Epoch [1/3], Step [1200/6471], Loss: 3.0839, Perplexity: 21.8423\n",
      "Epoch [1/3], Step [1300/6471], Loss: 2.7078, Perplexity: 14.9965\n",
      "Epoch [1/3], Step [1400/6471], Loss: 2.9434, Perplexity: 18.9802\n",
      "Epoch [1/3], Step [1500/6471], Loss: 2.9674, Perplexity: 19.4419\n",
      "Epoch [1/3], Step [1600/6471], Loss: 2.6412, Perplexity: 14.0302\n",
      "Epoch [1/3], Step [1700/6471], Loss: 2.6836, Perplexity: 14.6383\n",
      "Epoch [1/3], Step [1800/6471], Loss: 2.4515, Perplexity: 11.6056\n",
      "Epoch [1/3], Step [1900/6471], Loss: 2.6412, Perplexity: 14.0300\n",
      "Epoch [1/3], Step [2000/6471], Loss: 2.3957, Perplexity: 10.9760\n",
      "Epoch [1/3], Step [2100/6471], Loss: 2.5917, Perplexity: 13.3519\n",
      "Epoch [1/3], Step [2200/6471], Loss: 2.2714, Perplexity: 9.69316\n",
      "Epoch [1/3], Step [2300/6471], Loss: 2.4040, Perplexity: 11.0673\n",
      "Epoch [1/3], Step [2400/6471], Loss: 3.0551, Perplexity: 21.2234\n",
      "Epoch [1/3], Step [2500/6471], Loss: 2.2543, Perplexity: 9.52907\n",
      "Epoch [1/3], Step [2600/6471], Loss: 2.4890, Perplexity: 12.0490\n",
      "Epoch [1/3], Step [2700/6471], Loss: 2.3212, Perplexity: 10.1878\n",
      "Epoch [1/3], Step [2800/6471], Loss: 2.7672, Perplexity: 15.9142\n",
      "Epoch [1/3], Step [2900/6471], Loss: 2.6231, Perplexity: 13.7790\n",
      "Epoch [1/3], Step [3000/6471], Loss: 3.1691, Perplexity: 23.7864\n",
      "Epoch [1/3], Step [3100/6471], Loss: 2.2445, Perplexity: 9.43594\n",
      "Epoch [1/3], Step [3200/6471], Loss: 2.1450, Perplexity: 8.54236\n",
      "Epoch [1/3], Step [3300/6471], Loss: 2.5028, Perplexity: 12.2167\n",
      "Epoch [1/3], Step [3400/6471], Loss: 2.0451, Perplexity: 7.72980\n",
      "Epoch [1/3], Step [3500/6471], Loss: 2.0858, Perplexity: 8.05146\n",
      "Epoch [1/3], Step [3600/6471], Loss: 2.7365, Perplexity: 15.4336\n",
      "Epoch [1/3], Step [3700/6471], Loss: 2.4552, Perplexity: 11.6483\n",
      "Epoch [1/3], Step [3800/6471], Loss: 2.7563, Perplexity: 15.7419\n",
      "Epoch [1/3], Step [3900/6471], Loss: 2.2448, Perplexity: 9.43872\n",
      "Epoch [1/3], Step [4000/6471], Loss: 2.1258, Perplexity: 8.37939\n",
      "Epoch [1/3], Step [4100/6471], Loss: 2.3567, Perplexity: 10.5560\n",
      "Epoch [1/3], Step [4200/6471], Loss: 2.1980, Perplexity: 9.00715\n",
      "Epoch [1/3], Step [4300/6471], Loss: 2.4700, Perplexity: 11.8230\n",
      "Epoch [1/3], Step [4400/6471], Loss: 2.1677, Perplexity: 8.73824\n",
      "Epoch [1/3], Step [4500/6471], Loss: 2.1914, Perplexity: 8.94757\n",
      "Epoch [1/3], Step [4600/6471], Loss: 1.9258, Perplexity: 6.86081\n",
      "Epoch [1/3], Step [4700/6471], Loss: 2.3628, Perplexity: 10.6202\n",
      "Epoch [1/3], Step [4800/6471], Loss: 2.1068, Perplexity: 8.22216\n",
      "Epoch [1/3], Step [4900/6471], Loss: 2.2267, Perplexity: 9.26948\n",
      "Epoch [1/3], Step [5000/6471], Loss: 2.0621, Perplexity: 7.86211\n",
      "Epoch [1/3], Step [5100/6471], Loss: 2.5555, Perplexity: 12.8778\n",
      "Epoch [1/3], Step [5200/6471], Loss: 2.0568, Perplexity: 7.82092\n",
      "Epoch [1/3], Step [5300/6471], Loss: 2.1355, Perplexity: 8.46166\n",
      "Epoch [1/3], Step [5400/6471], Loss: 2.1572, Perplexity: 8.64671\n",
      "Epoch [1/3], Step [5500/6471], Loss: 2.0755, Perplexity: 7.96881\n",
      "Epoch [1/3], Step [5600/6471], Loss: 2.0956, Perplexity: 8.13041\n",
      "Epoch [1/3], Step [5700/6471], Loss: 2.1536, Perplexity: 8.61569\n",
      "Epoch [1/3], Step [5800/6471], Loss: 2.1584, Perplexity: 8.65757\n",
      "Epoch [1/3], Step [5900/6471], Loss: 2.5310, Perplexity: 12.5660\n",
      "Epoch [1/3], Step [6000/6471], Loss: 2.2170, Perplexity: 9.17994\n",
      "Epoch [1/3], Step [6100/6471], Loss: 2.0250, Perplexity: 7.57580\n",
      "Epoch [1/3], Step [6200/6471], Loss: 2.1042, Perplexity: 8.20099\n",
      "Epoch [1/3], Step [6300/6471], Loss: 2.7328, Perplexity: 15.3762\n",
      "Epoch [1/3], Step [6400/6471], Loss: 2.1250, Perplexity: 8.37297\n",
      "Epoch [2/3], Step [100/6471], Loss: 2.1205, Perplexity: 8.335010\n",
      "Epoch [2/3], Step [200/6471], Loss: 2.1356, Perplexity: 8.46182\n",
      "Epoch [2/3], Step [300/6471], Loss: 2.0745, Perplexity: 7.96092\n",
      "Epoch [2/3], Step [400/6471], Loss: 1.8562, Perplexity: 6.39962\n",
      "Epoch [2/3], Step [500/6471], Loss: 2.0346, Perplexity: 7.64911\n",
      "Epoch [2/3], Step [600/6471], Loss: 2.5062, Perplexity: 12.2583\n",
      "Epoch [2/3], Step [700/6471], Loss: 2.3805, Perplexity: 10.8101\n",
      "Epoch [2/3], Step [800/6471], Loss: 1.9759, Perplexity: 7.21348\n",
      "Epoch [2/3], Step [900/6471], Loss: 2.1692, Perplexity: 8.75135\n",
      "Epoch [2/3], Step [1000/6471], Loss: 2.8167, Perplexity: 16.7213\n",
      "Epoch [2/3], Step [1100/6471], Loss: 2.1182, Perplexity: 8.31644\n",
      "Epoch [2/3], Step [1200/6471], Loss: 2.2756, Perplexity: 9.73396\n",
      "Epoch [2/3], Step [1300/6471], Loss: 1.8677, Perplexity: 6.47354\n",
      "Epoch [2/3], Step [1400/6471], Loss: 2.1484, Perplexity: 8.57146\n",
      "Epoch [2/3], Step [1500/6471], Loss: 2.5499, Perplexity: 12.8064\n",
      "Epoch [2/3], Step [1600/6471], Loss: 2.0762, Perplexity: 7.97423\n",
      "Epoch [2/3], Step [1700/6471], Loss: 1.9533, Perplexity: 7.05221\n",
      "Epoch [2/3], Step [1800/6471], Loss: 2.0748, Perplexity: 7.96327\n",
      "Epoch [2/3], Step [1900/6471], Loss: 2.4275, Perplexity: 11.3303\n",
      "Epoch [2/3], Step [2000/6471], Loss: 2.0354, Perplexity: 7.65562\n",
      "Epoch [2/3], Step [2100/6471], Loss: 2.0616, Perplexity: 7.85896\n",
      "Epoch [2/3], Step [2200/6471], Loss: 2.1815, Perplexity: 8.85991\n",
      "Epoch [2/3], Step [2300/6471], Loss: 1.8111, Perplexity: 6.11725\n",
      "Epoch [2/3], Step [2400/6471], Loss: 2.0628, Perplexity: 7.86764\n",
      "Epoch [2/3], Step [2500/6471], Loss: 1.9919, Perplexity: 7.32934\n",
      "Epoch [2/3], Step [2600/6471], Loss: 2.1595, Perplexity: 8.66683\n",
      "Epoch [2/3], Step [2700/6471], Loss: 2.0758, Perplexity: 7.97067\n",
      "Epoch [2/3], Step [2800/6471], Loss: 1.9213, Perplexity: 6.82989\n",
      "Epoch [2/3], Step [2900/6471], Loss: 2.4243, Perplexity: 11.2945\n",
      "Epoch [2/3], Step [3000/6471], Loss: 2.1769, Perplexity: 8.81937\n",
      "Epoch [2/3], Step [3100/6471], Loss: 1.9346, Perplexity: 6.92169\n",
      "Epoch [2/3], Step [3200/6471], Loss: 2.0848, Perplexity: 8.04277\n",
      "Epoch [2/3], Step [3300/6471], Loss: 2.0719, Perplexity: 7.93992\n",
      "Epoch [2/3], Step [3400/6471], Loss: 2.7692, Perplexity: 15.9451\n",
      "Epoch [2/3], Step [3500/6471], Loss: 2.0222, Perplexity: 7.55487\n",
      "Epoch [2/3], Step [3600/6471], Loss: 2.1247, Perplexity: 8.37041\n",
      "Epoch [2/3], Step [3700/6471], Loss: 2.0040, Perplexity: 7.41879\n",
      "Epoch [2/3], Step [3800/6471], Loss: 2.0559, Perplexity: 7.81365\n",
      "Epoch [2/3], Step [3900/6471], Loss: 1.9326, Perplexity: 6.90730\n",
      "Epoch [2/3], Step [4000/6471], Loss: 1.9207, Perplexity: 6.82587\n",
      "Epoch [2/3], Step [4100/6471], Loss: 1.9282, Perplexity: 6.87692\n",
      "Epoch [2/3], Step [4200/6471], Loss: 2.0061, Perplexity: 7.43441\n",
      "Epoch [2/3], Step [4300/6471], Loss: 1.7992, Perplexity: 6.04506\n",
      "Epoch [2/3], Step [4400/6471], Loss: 2.0220, Perplexity: 7.55367\n",
      "Epoch [2/3], Step [4500/6471], Loss: 1.8270, Perplexity: 6.21557\n",
      "Epoch [2/3], Step [4600/6471], Loss: 2.0874, Perplexity: 8.06366\n",
      "Epoch [2/3], Step [4700/6471], Loss: 1.8389, Perplexity: 6.28965\n",
      "Epoch [2/3], Step [4800/6471], Loss: 2.0596, Perplexity: 7.84318\n",
      "Epoch [2/3], Step [4900/6471], Loss: 1.7025, Perplexity: 5.48798\n",
      "Epoch [2/3], Step [5000/6471], Loss: 2.1841, Perplexity: 8.88274\n",
      "Epoch [2/3], Step [5100/6471], Loss: 1.9465, Perplexity: 7.00394\n",
      "Epoch [2/3], Step [5200/6471], Loss: 2.0514, Perplexity: 7.77864\n",
      "Epoch [2/3], Step [5300/6471], Loss: 1.8189, Perplexity: 6.16542\n",
      "Epoch [2/3], Step [5400/6471], Loss: 1.7576, Perplexity: 5.79837\n",
      "Epoch [2/3], Step [5500/6471], Loss: 2.2200, Perplexity: 9.20743\n",
      "Epoch [2/3], Step [5600/6471], Loss: 1.8430, Perplexity: 6.31549\n",
      "Epoch [2/3], Step [5700/6471], Loss: 1.9508, Perplexity: 7.03410\n",
      "Epoch [2/3], Step [5800/6471], Loss: 1.9859, Perplexity: 7.28568\n",
      "Epoch [2/3], Step [5900/6471], Loss: 1.8490, Perplexity: 6.35337\n",
      "Epoch [2/3], Step [6000/6471], Loss: 1.9646, Perplexity: 7.13210\n",
      "Epoch [2/3], Step [6100/6471], Loss: 2.3025, Perplexity: 9.99916\n",
      "Epoch [2/3], Step [6200/6471], Loss: 1.9862, Perplexity: 7.28740\n",
      "Epoch [2/3], Step [6300/6471], Loss: 1.8637, Perplexity: 6.44734\n",
      "Epoch [2/3], Step [6400/6471], Loss: 2.0705, Perplexity: 7.92918\n",
      "Epoch [3/3], Step [100/6471], Loss: 1.8540, Perplexity: 6.385142\n",
      "Epoch [3/3], Step [200/6471], Loss: 2.0473, Perplexity: 7.74712\n",
      "Epoch [3/3], Step [300/6471], Loss: 1.9154, Perplexity: 6.78988\n",
      "Epoch [3/3], Step [400/6471], Loss: 2.2193, Perplexity: 9.20130\n",
      "Epoch [3/3], Step [500/6471], Loss: 2.0085, Perplexity: 7.45229\n",
      "Epoch [3/3], Step [600/6471], Loss: 2.2710, Perplexity: 9.68920\n",
      "Epoch [3/3], Step [700/6471], Loss: 1.8846, Perplexity: 6.58381\n",
      "Epoch [3/3], Step [800/6471], Loss: 1.9442, Perplexity: 6.98825\n",
      "Epoch [3/3], Step [900/6471], Loss: 2.0576, Perplexity: 7.82758\n",
      "Epoch [3/3], Step [1000/6471], Loss: 2.4081, Perplexity: 11.1133\n",
      "Epoch [3/3], Step [1100/6471], Loss: 2.0259, Perplexity: 7.58324\n",
      "Epoch [3/3], Step [1200/6471], Loss: 1.7884, Perplexity: 5.98006\n",
      "Epoch [3/3], Step [1300/6471], Loss: 2.0394, Perplexity: 7.68608\n",
      "Epoch [3/3], Step [1400/6471], Loss: 1.9217, Perplexity: 6.83243\n",
      "Epoch [3/3], Step [1500/6471], Loss: 2.1101, Perplexity: 8.24904\n",
      "Epoch [3/3], Step [1600/6471], Loss: 1.9638, Perplexity: 7.12659\n",
      "Epoch [3/3], Step [1700/6471], Loss: 1.8272, Perplexity: 6.21640\n",
      "Epoch [3/3], Step [1800/6471], Loss: 1.8171, Perplexity: 6.15400\n",
      "Epoch [3/3], Step [1900/6471], Loss: 1.9822, Perplexity: 7.25908\n",
      "Epoch [3/3], Step [2000/6471], Loss: 1.9440, Perplexity: 6.98679\n",
      "Epoch [3/3], Step [2100/6471], Loss: 1.7991, Perplexity: 6.04414\n",
      "Epoch [3/3], Step [2200/6471], Loss: 2.3206, Perplexity: 10.1815\n",
      "Epoch [3/3], Step [2300/6471], Loss: 1.8292, Perplexity: 6.22915\n",
      "Epoch [3/3], Step [2400/6471], Loss: 1.9309, Perplexity: 6.89580\n",
      "Epoch [3/3], Step [2500/6471], Loss: 1.7926, Perplexity: 6.00485\n",
      "Epoch [3/3], Step [2600/6471], Loss: 1.9474, Perplexity: 7.01078\n",
      "Epoch [3/3], Step [2700/6471], Loss: 2.0291, Perplexity: 7.60719\n",
      "Epoch [3/3], Step [2800/6471], Loss: 1.9210, Perplexity: 6.82783\n",
      "Epoch [3/3], Step [2900/6471], Loss: 1.9474, Perplexity: 7.01022\n",
      "Epoch [3/3], Step [3000/6471], Loss: 1.8867, Perplexity: 6.59757\n",
      "Epoch [3/3], Step [3100/6471], Loss: 1.9198, Perplexity: 6.81979\n",
      "Epoch [3/3], Step [3200/6471], Loss: 1.8517, Perplexity: 6.37086\n",
      "Epoch [3/3], Step [3300/6471], Loss: 1.8486, Perplexity: 6.35098\n",
      "Epoch [3/3], Step [3400/6471], Loss: 1.7560, Perplexity: 5.78953\n",
      "Epoch [3/3], Step [3500/6471], Loss: 1.8955, Perplexity: 6.65565\n",
      "Epoch [3/3], Step [3600/6471], Loss: 1.8449, Perplexity: 6.32744\n",
      "Epoch [3/3], Step [3700/6471], Loss: 1.8099, Perplexity: 6.10996\n",
      "Epoch [3/3], Step [3800/6471], Loss: 1.9788, Perplexity: 7.23448\n",
      "Epoch [3/3], Step [3900/6471], Loss: 1.8274, Perplexity: 6.21799\n",
      "Epoch [3/3], Step [4000/6471], Loss: 1.9353, Perplexity: 6.92583\n",
      "Epoch [3/3], Step [4100/6471], Loss: 2.4080, Perplexity: 11.1114\n",
      "Epoch [3/3], Step [4200/6471], Loss: 1.9109, Perplexity: 6.75890\n",
      "Epoch [3/3], Step [4300/6471], Loss: 1.9545, Perplexity: 7.06025\n",
      "Epoch [3/3], Step [4400/6471], Loss: 1.7778, Perplexity: 5.91668\n",
      "Epoch [3/3], Step [4500/6471], Loss: 1.7559, Perplexity: 5.78847\n",
      "Epoch [3/3], Step [4600/6471], Loss: 2.0438, Perplexity: 7.72001\n",
      "Epoch [3/3], Step [4700/6471], Loss: 1.8499, Perplexity: 6.35930\n",
      "Epoch [3/3], Step [4800/6471], Loss: 1.8442, Perplexity: 6.32322\n",
      "Epoch [3/3], Step [4900/6471], Loss: 1.8612, Perplexity: 6.43172\n",
      "Epoch [3/3], Step [5000/6471], Loss: 1.8212, Perplexity: 6.17952\n",
      "Epoch [3/3], Step [5100/6471], Loss: 1.8615, Perplexity: 6.43335\n",
      "Epoch [3/3], Step [5200/6471], Loss: 1.7703, Perplexity: 5.87262\n",
      "Epoch [3/3], Step [5300/6471], Loss: 1.8233, Perplexity: 6.19212\n",
      "Epoch [3/3], Step [5400/6471], Loss: 1.8531, Perplexity: 6.37958\n",
      "Epoch [3/3], Step [5500/6471], Loss: 1.8381, Perplexity: 6.28471\n",
      "Epoch [3/3], Step [5600/6471], Loss: 1.8844, Perplexity: 6.58234\n",
      "Epoch [3/3], Step [5700/6471], Loss: 1.9960, Perplexity: 7.35971\n",
      "Epoch [3/3], Step [5800/6471], Loss: 1.7279, Perplexity: 5.62885\n",
      "Epoch [3/3], Step [5900/6471], Loss: 1.8688, Perplexity: 6.48044\n",
      "Epoch [3/3], Step [6000/6471], Loss: 1.8192, Perplexity: 6.16702\n",
      "Epoch [3/3], Step [6100/6471], Loss: 2.1838, Perplexity: 8.88005\n",
      "Epoch [3/3], Step [6200/6471], Loss: 1.8563, Perplexity: 6.40027\n",
      "Epoch [3/3], Step [6300/6471], Loss: 1.8804, Perplexity: 6.55593\n",
      "Epoch [3/3], Step [6400/6471], Loss: 1.8267, Perplexity: 6.21347\n",
      "Epoch [3/3], Step [6471/6471], Loss: 1.8478, Perplexity: 6.34613"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
